{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import copy\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from collections import defaultdict, deque\n",
    "from typing import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.optim import Adam, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY_MOVE = (-1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe(gym.Env):\n",
    "    def __init__(self, n_rows, n_cols, n_win, clone=None):\n",
    "        self.gameOver = False\n",
    "        if clone is not None:\n",
    "            self.n_rows, self.n_cols, self.n_win = clone.n_rows, clone.n_cols, clone.n_win\n",
    "            self.board = copy.deepcopy(clone.board)\n",
    "            self.curTurn = clone.curTurn\n",
    "            self.emptySpaces = None\n",
    "            self.boardHash = None\n",
    "        else:\n",
    "            self.n_rows = n_rows\n",
    "            self.n_cols = n_cols\n",
    "            self.n_win = n_win\n",
    "\n",
    "            self.reset()\n",
    "\n",
    "    def getEmptySpaces(self):\n",
    "        if self.emptySpaces is None:\n",
    "            res = np.where(self.board == 0)\n",
    "            self.emptySpaces = np.array([(i, j) for i, j in zip(res[0], res[1])])\n",
    "        return self.emptySpaces\n",
    "\n",
    "    def makeMove(self, player, i, j):\n",
    "        self.board[i, j] = player\n",
    "        self.emptySpaces = None\n",
    "        self.boardHash = None\n",
    "\n",
    "    def getHash(self):\n",
    "        if self.boardHash is None:\n",
    "            self.boardHash = ''.join(['%s' % (x + 1) for x in self.board.reshape(self.n_rows * self.n_cols)])\n",
    "        return self.boardHash\n",
    "\n",
    "    def isTerminal(self):\n",
    "        # проверим, не закончилась ли игра\n",
    "        cur_marks, cur_p = np.where(self.board == self.curTurn), self.curTurn\n",
    "        for i, j in zip(cur_marks[0], cur_marks[1]):\n",
    "            win = False\n",
    "            if i <= self.n_rows - self.n_win:\n",
    "                if np.all(self.board[i:i + self.n_win, j] == cur_p):\n",
    "                    win = True\n",
    "            if not win:\n",
    "                if j <= self.n_cols - self.n_win:\n",
    "                    if np.all(self.board[i, j:j + self.n_win] == cur_p):\n",
    "                        win = True\n",
    "            if not win:\n",
    "                if i <= self.n_rows - self.n_win and j <= self.n_cols - self.n_win:\n",
    "                    if np.all(np.array([self.board[i + k, j + k] == cur_p for k in range(self.n_win)])):\n",
    "                        win = True\n",
    "            if not win:\n",
    "                if i <= self.n_rows - self.n_win and j >= self.n_win - 1:\n",
    "                    if np.all(np.array([self.board[i + k, j - k] == cur_p for k in range(self.n_win)])):\n",
    "                        win = True\n",
    "            if win:\n",
    "                self.gameOver = True\n",
    "                return self.curTurn\n",
    "\n",
    "        if len(self.getEmptySpaces()) == 0:\n",
    "            self.gameOver = True\n",
    "            return 0\n",
    "\n",
    "        self.gameOver = False\n",
    "        return None\n",
    "\n",
    "    def printBoard(self):\n",
    "        for i in range(0, self.n_rows):\n",
    "            print('----' * self.n_cols + '-')\n",
    "            out = '| '\n",
    "            for j in range(0, self.n_cols):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                else:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('----' * self.n_cols + '-')\n",
    "\n",
    "    def getState(self):\n",
    "        return self.getHash(), self.getEmptySpaces(), self.curTurn\n",
    "\n",
    "    def action_from_int(self, action_int):\n",
    "        return int(action_int / self.n_cols), int(action_int % self.n_cols)\n",
    "\n",
    "    def int_from_action(self, action):\n",
    "        return action[0] * self.n_cols + action[1]\n",
    "\n",
    "    def step(self, action: Sequence[int]):\n",
    "        if self.board[action[0], action[1]] != 0:\n",
    "            self.curTurn = -self.curTurn\n",
    "            return self.getState(), -10, True, {}\n",
    "        self.makeMove(self.curTurn, action[0], action[1])\n",
    "        reward = self.isTerminal()\n",
    "        self.curTurn = -self.curTurn\n",
    "        return self.getState(), 0 if reward is None else reward, reward is not None, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
    "        self.boardHash = None\n",
    "        self.gameOver = False\n",
    "        self.emptySpaces = None\n",
    "        self.curTurn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AgentQ:\n",
    "    def __init__(self, alpha=0.05, gamma=0.9):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.q = defaultdict(dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def state_to_int(state):\n",
    "        return int(''.join(['%s' % (x + 1) for x in state.ravel()]), 3)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_spaces(state, done):\n",
    "        res = np.where(state == 0)\n",
    "        if len(res[0]) == 0 or done:\n",
    "            return [EMPTY_MOVE]\n",
    "        else:\n",
    "            return [(i, j) for i, j in zip(res[0], res[1])]\n",
    "\n",
    "    def get_policy(self):\n",
    "        policy = {}\n",
    "        for state, action_dict in self.q.items():\n",
    "            best_action = max(action_dict.items(), key=lambda x: x[1])[0]\n",
    "            policy[state] = best_action\n",
    "        return policy\n",
    "\n",
    "    def init_q(self, state, empty_spaces):\n",
    "        if state not in self.q:\n",
    "            actions = [tuple(map(int, action)) for action in empty_spaces]\n",
    "            self.q[state] = {action: np.float16(np.random.random()) for action in actions}\n",
    "\n",
    "    def act(self, state):\n",
    "        empty_spaces = self.get_empty_spaces(state, False)\n",
    "        state = self.state_to_int(state)\n",
    "        self.init_q(state, empty_spaces)\n",
    "        return max(self.q[state].items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    def update(self, state, action, next_state, reward, done):\n",
    "        empty_spaces = self.get_empty_spaces(state, False)\n",
    "        state = self.state_to_int(state)\n",
    "        self.init_q(state, empty_spaces)\n",
    "        if next_state is None:\n",
    "            self.q[state][action] = reward\n",
    "        else:\n",
    "            empty_spaces = self.get_empty_spaces(next_state, done)\n",
    "            next_state = self.state_to_int(next_state)\n",
    "            if done:\n",
    "                next_state_value = 0\n",
    "            else:\n",
    "                self.init_q(next_state, empty_spaces)\n",
    "                next_state_value = max(self.q[next_state].values())\n",
    "            self.q[state][action] = self.q[state][action] + self.alpha * (\n",
    "                reward\n",
    "                + self.gamma * next_state_value\n",
    "                - self.q[state][action]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, env: TicTacToe, agent_cross, agent_zero, epsilon=0.2):\n",
    "        self.env = env\n",
    "        self.agent_cross = agent_cross\n",
    "        self.agent_zeros = agent_zero\n",
    "        self.epsilon = epsilon\n",
    "        self.cross_s = None\n",
    "        self.cross_a = None\n",
    "        self.zeros_s = None\n",
    "        self.zeros_a = None\n",
    "        self.prime_cross_s = None\n",
    "        self.prime_cross_a = None\n",
    "        self.prime_zeros_s = None\n",
    "        self.prime_zeros_a = None\n",
    "        self.done = False\n",
    "        self.reward_zeros = 0\n",
    "        self.reward_cross = 0\n",
    "        self.cur_turn = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.done = False\n",
    "        self.cross_s = self.env.board.copy()\n",
    "        self.cross_a = self.agent_cross.act(self.env.board)\n",
    "        self.zeros_s = None\n",
    "        self.zeros_a = None\n",
    "        self.prime_cross_s = None\n",
    "        self.prime_zeros_s = None\n",
    "        self.prime_cross_a = None\n",
    "        self.prime_zeros_a = None\n",
    "        self.reward_zeros = 0\n",
    "        self.reward_cross = 0\n",
    "        self.cur_turn = 1\n",
    "\n",
    "    def set_action(self):\n",
    "        if self.cur_turn == 1:\n",
    "            self.prime_cross_a = self.cross_a\n",
    "            if self.done:\n",
    "                self.cross_a = EMPTY_MOVE\n",
    "            elif random.random() > self.epsilon:\n",
    "                self.cross_a = self.agent_cross.act(self.cross_s)\n",
    "            else:\n",
    "                self.cross_a = tuple(random.choice(self.env.getEmptySpaces()))\n",
    "        else:\n",
    "            self.prime_zeros_a = self.zeros_a\n",
    "            if self.done:\n",
    "                self.zeros_a = EMPTY_MOVE\n",
    "            elif random.random() > self.epsilon:\n",
    "                self.zeros_a = self.agent_zeros.act(self.zeros_s)\n",
    "            else:\n",
    "                self.zeros_a = tuple(random.choice(self.env.getEmptySpaces()))\n",
    "\n",
    "    def parse_reward(self, reward):\n",
    "        self.reward_cross = reward\n",
    "        self.reward_zeros = -reward\n",
    "        if reward == -10:\n",
    "            if self.cur_turn == 1:\n",
    "                self.reward_cross = 0\n",
    "                self.reward_zeros = -10\n",
    "            else:\n",
    "                self.reward_cross = -10\n",
    "                self.reward_zeros = 0\n",
    "\n",
    "    def env_step(self):\n",
    "        if self.cur_turn == 1:\n",
    "            self.prime_zeros_s = self.zeros_s\n",
    "            _, reward, self.done, _ = self.env.step(self.cross_a)\n",
    "            self.cur_turn = self.env.curTurn\n",
    "            self.zeros_s = self.env.board.copy()\n",
    "            self.set_action()\n",
    "            self.parse_reward(reward)\n",
    "            if self.prime_zeros_s is not None and self.prime_zeros_a is not None:\n",
    "                self.agent_zeros.update(\n",
    "                    self.prime_zeros_s,\n",
    "                    self.prime_zeros_a,\n",
    "                    self.zeros_s,\n",
    "                    self.reward_zeros,\n",
    "                    self.done,\n",
    "                )\n",
    "            if self.done:\n",
    "                self.agent_cross.update(\n",
    "                    self.cross_s, self.cross_a, None, self.reward_cross, self.done\n",
    "                )\n",
    "        else:\n",
    "            self.prime_cross_s = self.cross_s\n",
    "            _, reward, self.done, _ = self.env.step(self.zeros_a)\n",
    "            self.cur_turn = self.env.curTurn\n",
    "            self.cross_s = self.env.board.copy()\n",
    "            self.set_action()\n",
    "            self.parse_reward(reward)\n",
    "            if self.prime_cross_a is not None:\n",
    "                self.agent_cross.update(\n",
    "                    self.prime_cross_s,\n",
    "                    self.prime_cross_a,\n",
    "                    self.cross_s,\n",
    "                    self.reward_cross,\n",
    "                    self.done,\n",
    "                )\n",
    "            if self.done:\n",
    "                self.agent_zeros.update(\n",
    "                    self.zeros_s, self.zeros_a, None, self.reward_zeros, self.done\n",
    "                )\n",
    "\n",
    "    def evaluate_policy(self):\n",
    "        cross_rewards = []\n",
    "        zeros_rewards = []\n",
    "        for i in range(1000):\n",
    "            self.reset()\n",
    "            done = False\n",
    "            reward = 0\n",
    "            while not done:\n",
    "                if self.env.curTurn == 1:\n",
    "                    state_c = self.env.board\n",
    "                    action_c = self.agent_cross.act(state_c)\n",
    "                    _, reward, done, _ = self.env.step(action_c)\n",
    "                else:\n",
    "                    action_z = random.choice(self.env.getEmptySpaces())\n",
    "                    _, reward, done, _ = self.env.step(action_z)\n",
    "            self.parse_reward(reward)\n",
    "            cross_rewards.append(self.reward_cross)\n",
    "            self.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                if self.env.curTurn == -1:\n",
    "                    state_z = self.env.board\n",
    "                    action_z = self.agent_zeros.act(state_z)\n",
    "                    _, reward, done, _ = self.env.step(action_z)\n",
    "                else:\n",
    "                    action_c = random.choice(self.env.getEmptySpaces())\n",
    "                    _, reward, done, _ = self.env.step(action_c)\n",
    "            self.parse_reward(reward)\n",
    "            zeros_rewards.append(self.reward_zeros)\n",
    "        return np.mean(cross_rewards), np.mean(zeros_rewards)\n",
    "\n",
    "    def train(self, iterations):\n",
    "        rewards_cross = []\n",
    "        rewards_zeros = []\n",
    "        iters = []\n",
    "        for i in range(iterations):\n",
    "            self.reset()\n",
    "\n",
    "            while not self.done:\n",
    "                self.env_step()\n",
    "            if (i + 1) % 5000 == 0:\n",
    "                mean_cross_reward, mean_zeros_reward = self.evaluate_policy()\n",
    "                rewards_cross.append(mean_cross_reward)\n",
    "                rewards_zeros.append(mean_zeros_reward)\n",
    "                iters.append(i + 1)\n",
    "                if (i + 1) % 20000 == 0:\n",
    "                    print(f\"Iter {i + 1}/{iterations}, cross reward: {mean_cross_reward}, zeros reward {mean_zeros_reward}\")\n",
    "        return iters, rewards_cross, rewards_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Реализуйте обычное (табличное) Q-обучение. Обучите стратегии крестиков и ноликов для доски 3х3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43254/1513455434.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0magent_zeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgentQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_cross\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_zeros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_cross\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_zeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{N_ROWS}_q_policy_cross.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_43254/1666427442.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, iterations)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mmean_cross_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_zeros_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_43254/1666427442.py\u001b[0m in \u001b[0;36menv_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_turn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprime_zeros_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_turn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurTurn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_43254/4185067609.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakeMove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurTurn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misTerminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurTurn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurTurn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_43254/4185067609.py\u001b[0m in \u001b[0;36misTerminal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misTerminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# проверим, не закончилась ли игра\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mcur_marks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurTurn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurTurn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_marks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_marks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mwin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_COLS, N_ROWS, N_WINS = 3, 3, 3\n",
    "\n",
    "env = TicTacToe(N_ROWS, N_COLS, N_WINS)\n",
    "agent_cross = AgentQ()\n",
    "agent_zeros = AgentQ()\n",
    "trainer = Trainer(env, agent_cross, agent_zeros)\n",
    "iterations, rewards_cross, rewards_zeros = trainer.train(100_000)\n",
    "\n",
    "with open(f\"{N_ROWS}_q_policy_cross.json\", \"w\") as f:\n",
    "    json.dump(agent_cross.get_policy(), f)\n",
    "with open(f\"{N_ROWS}_q_policy_zeros.json\", \"w\") as f:\n",
    "    json.dump(agent_zeros.get_policy(), f)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.plot(iterations, rewards_cross, label=\"cross rewards\")\n",
    "ax.plot(iterations, rewards_zeros, label=\"zeros rewards\")\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"mean reward\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Попробуйте обучить стратегии крестиков и ноликов для доски 4х4 и/или 5х5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COLS, N_ROWS, N_WINS = 4, 4, 4\n",
    "\n",
    "env = TicTacToe(N_ROWS, N_COLS, N_WINS)\n",
    "agent_cross = AgentQ()\n",
    "agent_zeros = AgentQ()\n",
    "trainer = Trainer(env, agent_cross, agent_zeros)\n",
    "iterations, rewards_cross, rewards_zeros = trainer.train(3_000_000)\n",
    "\n",
    "with open(f\"{N_ROWS}_q_policy_cross.json\", \"w\") as f:\n",
    "    json.dump(agent_cross.get_policy(), f)\n",
    "with open(f\"{N_ROWS}_q_policy_zeros.json\", \"w\") as f:\n",
    "    json.dump(agent_zeros.get_policy(), f)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.plot(iterations, rewards_cross, label=\"cross rewards\")\n",
    "ax.plot(iterations, rewards_zeros, label=\"zeros rewards\")\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"mean reward\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Реализуйте DQN с нейронной сетью, обучите стратегии крестиков и ноликов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=(3, 3), padding=(1, 1)),\n",
    "        )\n",
    "        self.head = nn.Linear(state_dim * 16, state_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x).view(x.shape[0], -1)\n",
    "        return self.head(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpirienceReplay(deque):\n",
    "    def sample(self, size):\n",
    "        batch = random.choices(self, k=size)\n",
    "        return list(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        gamma=0.99,\n",
    "        buffer_size=128_000,\n",
    "        learning_rate=1e-3,\n",
    "        device=\"cuda\",\n",
    "        batch_size=128,\n",
    "        initial_steps=1024,\n",
    "        steps_per_update=4\n",
    "    ):\n",
    "        self.steps = 0\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.initial_steps = initial_steps\n",
    "        self.steps_per_update = steps_per_update\n",
    "        self.model = QModel(state_dim).to(device)\n",
    "        self.buffer = ExpirienceReplay(maxlen=buffer_size)\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.criteria = nn.MSELoss()\n",
    "        self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size=10000, gamma=0.8)\n",
    "\n",
    "    def consume_transition(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = self.buffer.sample(self.batch_size)\n",
    "        state, action, next_state, reward, done = batch\n",
    "        state = torch.tensor(np.array(state, dtype=np.float32)).unsqueeze(1)\n",
    "        action = torch.tensor(np.array(action, dtype=np.int64))\n",
    "        next_state = torch.tensor(np.array(next_state, dtype=np.float32)).unsqueeze(1)\n",
    "        reward = torch.tensor(np.array(reward, dtype=np.float32))\n",
    "        done = torch.tensor(np.array(done, dtype=np.int32))\n",
    "        return state, action, next_state, reward, done\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        if not self.model.training:\n",
    "            self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        state, action, next_state, reward, done = batch\n",
    "        current_q = self.model(state.to(self.device))\n",
    "        next_q = self.model(next_state.to(self.device))\n",
    "        next_action = torch.argmax(next_q, 1)\n",
    "        next_target_q = self.model(next_state.to(self.device))\n",
    "        action_reward = current_q.gather(1, action.view(-1, 1).to(self.device))\n",
    "        next_actions_reward = next_target_q.gather(1, next_action.view(-1, 1))\n",
    "        next_actions_reward = next_actions_reward.squeeze(1) * (\n",
    "            1 - done.to(self.device)\n",
    "        )\n",
    "        loss = self.criteria(\n",
    "            action_reward.squeeze(1),\n",
    "            reward.to(self.device) + self.gamma * next_actions_reward,\n",
    "        )\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.steps > 1_000_000:\n",
    "            self.scheduler.step(loss)\n",
    "\n",
    "    def act(self, state):\n",
    "        if self.model.training:\n",
    "            self.model.eval()\n",
    "        state = torch.tensor(np.array(state)).view(1, 1, state.shape[-2], state.shape[-1])\n",
    "        state = state.float().to(self.device)\n",
    "        action_rewards = self.model(state).detach().cpu().numpy()\n",
    "        index = np.argmax(action_rewards.squeeze())\n",
    "        return index // state.shape[-1], index % state.shape[-1]\n",
    "\n",
    "    def update(self, state, action, next_state, reward, done):\n",
    "        if next_state is None:\n",
    "            next_state = state\n",
    "        action = action[0] * state.shape[-1] + action[1]\n",
    "        self.consume_transition((state, action, next_state, reward, done))\n",
    "        self.steps += 1\n",
    "        if self.steps > self.initial_steps:\n",
    "            if self.steps % self.steps_per_update == 0:\n",
    "                batch = self.sample_batch()\n",
    "                self.train_step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WINS = 3, 3, 3\n",
    "\n",
    "env = TicTacToe(N_ROWS, N_COLS, N_WINS)\n",
    "agent_cross = AgentDQN(N_ROWS * N_COLS, device=\"cuda\")\n",
    "agent_zeros = AgentDQN(N_ROWS * N_COLS, device=\"cuda\")\n",
    "trainer = Trainer(env, agent_cross, agent_zeros)\n",
    "iterations, rewards_cross, rewards_zeros = trainer.train(100_000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.plot(iterations, rewards_cross, label=\"cross rewards\")\n",
    "ax.plot(iterations, rewards_zeros, label=\"zeros rewards\")\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"mean reward\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WINS = 4, 4, 4\n",
    "\n",
    "env = TicTacToe(N_ROWS, N_COLS, N_WINS)\n",
    "agent_cross = AgentDQN(N_ROWS * N_COLS, device=\"cuda\")\n",
    "agent_zeros = AgentDQN(N_ROWS * N_COLS, device=\"cuda\")\n",
    "trainer = Trainer(env, agent_cross, agent_zeros)\n",
    "iterations, rewards_cross, rewards_zeros = trainer.train(50_000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.plot(iterations, rewards_cross, label=\"cross rewards\")\n",
    "ax.plot(iterations, rewards_zeros, label=\"zeros rewards\")\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"mean reward\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WINS = 5, 5, 5\n",
    "\n",
    "env = TicTacToe(N_ROWS, N_COLS, N_WINS)\n",
    "agent_cross = AgentDQN(N_ROWS * N_COLS, device=\"cuda\")\n",
    "agent_zeros = AgentDQN(N_ROWS * N_COLS, device=\"cuda\")\n",
    "trainer = Trainer(env, agent_cross, agent_zeros)\n",
    "iterations, rewards_cross, rewards_zeros = trainer.train(50_000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.plot(iterations, rewards_cross, label=\"cross rewards\")\n",
    "ax.plot(iterations, rewards_zeros, label=\"zeros rewards\")\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"mean reward\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Реализуйте Double DQN и/или Dueling DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDDQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        gamma=0.99,\n",
    "        buffer_size=128_000,\n",
    "        learning_rate=1e-3,\n",
    "        device=\"cuda\",\n",
    "        batch_size=128,\n",
    "        initial_steps=1024,\n",
    "        steps_per_update=4,\n",
    "        steps_per_target_update=4000,\n",
    "    ):\n",
    "        self.steps = 0  # Do not change\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.initial_steps = initial_steps\n",
    "        self.steps_per_update = steps_per_update\n",
    "        self.steps_per_target_update = steps_per_target_update\n",
    "        self.model = QModel(state_dim).to(device)  # Torch model\n",
    "        self.target_model = QModel(state_dim).to(device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()\n",
    "        self.buffer = ExpirienceReplay(maxlen=buffer_size)\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.criteria = nn.MSELoss()\n",
    "        self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size=10000, gamma=0.8)\n",
    "\n",
    "    def consume_transition(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = self.buffer.sample(self.batch_size)\n",
    "        state, action, next_state, reward, done = batch\n",
    "        state = torch.tensor(np.array(state, dtype=np.float32)).unsqueeze(1)\n",
    "        action = torch.tensor(np.array(action, dtype=np.int64))\n",
    "        next_state = torch.tensor(np.array(next_state, dtype=np.float32)).unsqueeze(1)\n",
    "        reward = torch.tensor(np.array(reward, dtype=np.float32))\n",
    "        done = torch.tensor(np.array(done, dtype=np.int32))\n",
    "        return state, action, next_state, reward, done\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        if not self.model.training:\n",
    "            self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        state, action, next_state, reward, done = batch\n",
    "        current_q = self.model(state.to(self.device))\n",
    "        next_q = self.model(next_state.to(self.device))\n",
    "        next_action = torch.argmax(next_q, 1)\n",
    "        with torch.no_grad():\n",
    "            next_target_q = self.target_model(next_state.to(self.device))\n",
    "        action_reward = current_q.gather(1, action.view(-1, 1).to(self.device))\n",
    "        next_actions_reward = next_target_q.gather(1, next_action.view(-1, 1))\n",
    "        next_actions_reward = next_actions_reward.squeeze(1) * (\n",
    "            1 - done.to(self.device)\n",
    "        )\n",
    "        loss = self.criteria(\n",
    "            action_reward.squeeze(1),\n",
    "            reward.to(self.device) + self.gamma * next_actions_reward,\n",
    "        )\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.steps > 1_000_000:\n",
    "            self.scheduler.step(loss)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "    def act(self, state):\n",
    "        if self.model.training:\n",
    "            self.model.eval()\n",
    "        state = torch.tensor(np.array(state)).view(1, 1, state.shape[-2], state.shape[-1])\n",
    "        state = state.float().to(self.device)\n",
    "        action_rewards = self.model(state).detach().cpu().numpy()\n",
    "        index = int(np.argmax(action_rewards.squeeze()))\n",
    "        return index // state.shape[-1], index % state.shape[-1]\n",
    "\n",
    "    def update(self, state, action, next_state, reward, done):\n",
    "        if next_state is None:\n",
    "            next_state = state\n",
    "        action = action[0] * state.shape[-1] + action[1]\n",
    "        self.consume_transition((state, action, next_state, reward, done))\n",
    "        self.steps += 1\n",
    "        if self.steps > self.initial_steps:\n",
    "            if self.steps % self.steps_per_update == 0:\n",
    "                batch = self.sample_batch()\n",
    "                self.train_step(batch)\n",
    "            if self.steps % self.steps_per_target_update == 0:\n",
    "                self.update_target_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WINS = 3, 3, 3\n",
    "\n",
    "env = TicTacToe(N_ROWS, N_COLS, N_WINS)\n",
    "agent_cross = AgentDDQN(N_ROWS * N_COLS, device=\"cuda\")\n",
    "agent_zeros = AgentDDQN(N_ROWS * N_COLS, device=\"cuda\")\n",
    "trainer = Trainer(env, agent_cross, agent_zeros)\n",
    "iterations, rewards_cross, rewards_zeros = trainer.train(50_000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.plot(iterations, rewards_cross, label=\"cross rewards\")\n",
    "ax.plot(iterations, rewards_zeros, label=\"zeros rewards\")\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"mean reward\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WINS = 4, 4, 4\n",
    "\n",
    "env = TicTacToe(N_ROWS, N_COLS, N_WINS)\n",
    "agent_cross = AgentDDQN(N_ROWS * N_COLS, device=\"cuda\")\n",
    "agent_zeros = AgentDDQN(N_ROWS * N_COLS, device=\"cuda\")\n",
    "trainer = Trainer(env, agent_cross, agent_zeros)\n",
    "iterations, rewards_cross, rewards_zeros = trainer.train(50_000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.plot(iterations, rewards_cross, label=\"cross rewards\")\n",
    "ax.plot(iterations, rewards_zeros, label=\"zeros rewards\")\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"mean reward\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WINS = 5, 5, 5\n",
    "\n",
    "env = TicTacToe(N_ROWS, N_COLS, N_WINS)\n",
    "agent_cross = AgentDDQN(N_ROWS * N_COLS, device=\"cuda\")\n",
    "agent_zeros = AgentDDQN(N_ROWS * N_COLS, device=\"cuda\")\n",
    "trainer = Trainer(env, agent_cross, agent_zeros)\n",
    "iterations, rewards_cross, rewards_zeros = trainer.train(50_000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.plot(iterations, rewards_cross, label=\"cross rewards\")\n",
    "ax.plot(iterations, rewards_zeros, label=\"zeros rewards\")\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"mean reward\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 -Реализуйте rollouts со случайной стратегией и (опционально) rollouts с неслучайной, но простой стратегией (например, основанной на дополнении нескольких паттернов или на Q-функции, которая у вас получилась в первом пункте)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пустая политика даст случайный rollout\n",
    "\n",
    "def rollout(env, policy_cross, policy_zeros):\n",
    "    reward = 0\n",
    "    done = False\n",
    "    clone_env = TicTacToe(3, 3, 3, clone=env)\n",
    "    while not done:\n",
    "        state = state_to_int(env.board)\n",
    "        if env.curTurn == 1 and state in policy_cross:\n",
    "            move = policy_cross[state]\n",
    "        elif env.curTurn == 0 and state in policy_zeros:\n",
    "            move = policy_zeros[state]\n",
    "        else:\n",
    "            move = random.choice(clone_env.getEmptySpaces())\n",
    "        _, reward, done, _ = clone_env.step(move)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Реализуйте MCTS-поиск с этими rollouts для крестиков-ноликов на досках разного размера, сравните полученные стратегии между собой и со стратегиями, обученными в первых двух частях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, move=None, parent=None, empty_spaces=None, turn=None, c=2):\n",
    "        self.move = move\n",
    "        self.parentNode = parent\n",
    "        self.childNodes = {}\n",
    "        self.wins = 0\n",
    "        self.visits = 0\n",
    "        self.c = c\n",
    "        self.untriedMoves = list(map(tuple, empty_spaces))\n",
    "        self.turn = turn\n",
    "\n",
    "    def utc(self, node, visits):\n",
    "        return node.wins / node.visits + self.c * np.sqrt(2 * np.log(visits) / node.visits)\n",
    "\n",
    "    def utc_select_child(self):\n",
    "        s = max(self.childNodes.values(), key=lambda x: self.utc(x, self.visits))\n",
    "        return s\n",
    "\n",
    "    def random_select(self):\n",
    "        s = random.choice(list(self.childNodes.values()))\n",
    "        return s\n",
    "\n",
    "    def add_child(self, move, empty_spaces, turn):\n",
    "\n",
    "        node = Node(\n",
    "            move=move,\n",
    "            parent=self,\n",
    "            empty_spaces=empty_spaces,\n",
    "            turn=turn,\n",
    "        )\n",
    "        self.untriedMoves.remove(move)\n",
    "        self.childNodes[move] = node\n",
    "        return node\n",
    "\n",
    "    def update(self, result):\n",
    "        self.visits += 1\n",
    "        self.wins += result * self.turn\n",
    "\n",
    "    def update_untried_moves(self, state, empty_spaces):\n",
    "        if state not in self.untriedMoves:\n",
    "            self.untriedMoves[state] = list(map(tuple, empty_spaces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_int(state):\n",
    "    return int(''.join(['%s' % (x + 1) for x in state.ravel()]), 3)\n",
    "\n",
    "\n",
    "def get_empty_spaces(state, done):\n",
    "    res = np.where(state == 0)\n",
    "    if len(res[0]) == 0 or done:\n",
    "        return []\n",
    "    else:\n",
    "        return [(i, j) for i, j in zip(res[0], res[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_games(env, policy, turn, iterations=1000):\n",
    "    env = TicTacToe(3, 3, 3, clone=env)\n",
    "    rewards = []\n",
    "    root = policy\n",
    "    for _ in range(iterations):\n",
    "        reward = 0\n",
    "        env.reset()\n",
    "        done = False\n",
    "        policy_exists = True\n",
    "        policy = root\n",
    "        while not done:\n",
    "            if env.curTurn == turn and len(policy.childNodes) > 0 and policy_exists:\n",
    "                policy = max(policy.childNodes.values(), key=lambda x: x.wins / x.visits)\n",
    "                _, reward, done, _ = env.step(policy.move)\n",
    "            elif env.curTurn == turn:\n",
    "                move = random.choice(get_empty_spaces(env.board, env.gameOver))\n",
    "                _, reward, done, _ = env.step(move)\n",
    "            else:\n",
    "                move = random.choice(get_empty_spaces(env.board, env.gameOver))\n",
    "                _, reward, done, _ = env.step(move)\n",
    "                if move in policy.childNodes:\n",
    "                    policy = policy.childNodes[move]\n",
    "                else:\n",
    "                    policy_exists = False\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mcts(env: TicTacToe, itermax, policy_cross, policy_zeros):\n",
    "\n",
    "    rootnode = Node(empty_spaces=env.getEmptySpaces(), turn=0)\n",
    "    cross_rewards = []\n",
    "    zeros_rewards = []\n",
    "    iterations = []\n",
    "    log_step = itermax // 10\n",
    "    for i in range(itermax):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        env.reset()\n",
    "        node = rootnode\n",
    "\n",
    "        # select leaf\n",
    "        while len(node.untriedMoves) == 0 and len(node.childNodes) > 0:\n",
    "            node = node.utc_select_child()\n",
    "            _, reward, done, _ = env.step(node.move)\n",
    "\n",
    "        # expand leaf\n",
    "        if len(node.untriedMoves) > 0:\n",
    "            move = random.choice(node.untriedMoves)\n",
    "            cur_turn = env.curTurn\n",
    "            _, reward, done, _ = env.step(move)\n",
    "            node = node.add_child(\n",
    "                move=move,\n",
    "                empty_spaces=get_empty_spaces(env.board, env.gameOver),\n",
    "                turn=cur_turn\n",
    "            )\n",
    "\n",
    "        # rollout\n",
    "        if not done:\n",
    "            reward = rollout(env, policy_cross, policy_zeros)\n",
    "\n",
    "        # backprop\n",
    "        while node is not None:\n",
    "            node.update(reward)\n",
    "            node = node.parentNode\n",
    "\n",
    "        if (i + 1) % log_step == 0:\n",
    "            cross_reward = simulate_games(env, rootnode, 1)\n",
    "            zeros_reward = -simulate_games(env, rootnode, -1)\n",
    "            cross_rewards.append(cross_reward)\n",
    "            zeros_rewards.append(zeros_reward)\n",
    "            iterations.append(i + 1)\n",
    "            print(f\"Iteration {i + 1}, cross reward: {cross_reward}, zeros reward: {zeros_reward}\")\n",
    "\n",
    "    return rootnode, iterations, cross_rewards, zeros_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WINS = 3, 3, 3\n",
    "\n",
    "with open(f\"{N_ROWS}_q_policy_cross.json\") as f:\n",
    "    policy_cross = json.load(f)\n",
    "with open(f\"{N_ROWS}_q_policy_zeros.json\") as f:\n",
    "    policy_zeros = json.load(f)\n",
    "\n",
    "env = TicTacToe(N_ROWS, N_COLS, N_WINS)\n",
    "tree, iterations, rewards_cross, rewards_zeros = train_mcts(env, 500_000, policy_cross, policy_zeros)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.plot(iterations, rewards_cross, label=\"cross rewards\")\n",
    "ax.plot(iterations, rewards_zeros, label=\"zeros rewards\")\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"mean reward\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WINS = 4, 4, 4\n",
    "\n",
    "with open(f\"{N_ROWS}_q_policy_cross.json\") as f:\n",
    "    policy_cross = json.load(f)\n",
    "with open(f\"{N_ROWS}_q_policy_zeros.json\") as f:\n",
    "    policy_zeros = json.load(f)\n",
    "\n",
    "env = TicTacToe(N_ROWS, N_COLS, N_WINS)\n",
    "tree, iterations, rewards_cross, rewards_zeros = train_mcts(env, 10_000_000, policy_cross, policy_zeros)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(16)\n",
    "ax.plot(iterations, rewards_cross, label=\"cross rewards\")\n",
    "ax.plot(iterations, rewards_zeros, label=\"zeros rewards\")\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"mean reward\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "395bb32cbcecf10a878680820ab516cd6c613bfc5f39a5087bea9c6c2db8bbc7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
