{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe(gym.Env):\n",
    "    def __init__(self, n_rows, n_cols, n_win, clone=None):\n",
    "        if clone is not None:\n",
    "            self.n_rows, self.n_cols, self.n_win = clone.n_rows, clone.n_cols, clone.n_win\n",
    "            self.board = copy.deepcopy(clone.board)\n",
    "            self.curTurn = clone.curTurn\n",
    "            self.emptySpaces = None\n",
    "            self.boardHash = None\n",
    "        else:\n",
    "            self.n_rows = n_rows\n",
    "            self.n_cols = n_cols\n",
    "            self.n_win = n_win\n",
    "\n",
    "            self.reset()\n",
    "\n",
    "    def getEmptySpaces(self):\n",
    "        if self.emptySpaces is None:\n",
    "            res = np.where(self.board == 0)\n",
    "            self.emptySpaces = np.array([ (i, j) for i,j in zip(res[0], res[1]) ])\n",
    "        return self.emptySpaces\n",
    "\n",
    "    def makeMove(self, player, i, j):\n",
    "        self.board[i, j] = player\n",
    "        self.emptySpaces = None\n",
    "        self.boardHash = None\n",
    "\n",
    "    def getHash(self):\n",
    "        if self.boardHash is None:\n",
    "            self.boardHash = ''.join(['%s' % (x+1) for x in self.board.reshape(self.n_rows * self.n_cols)])\n",
    "        return self.boardHash\n",
    "\n",
    "    def isTerminal(self):\n",
    "        # проверим, не закончилась ли игра\n",
    "        cur_marks, cur_p = np.where(self.board == self.curTurn), self.curTurn\n",
    "        for i,j in zip(cur_marks[0], cur_marks[1]):\n",
    "            win = False\n",
    "            if i <= self.n_rows - self.n_win:\n",
    "                if np.all(self.board[i:i+self.n_win, j] == cur_p):\n",
    "                    win = True\n",
    "            if not win:\n",
    "                if j <= self.n_cols - self.n_win:\n",
    "                    if np.all(self.board[i,j:j+self.n_win] == cur_p):\n",
    "                        win = True\n",
    "            if not win:\n",
    "                if i <= self.n_rows - self.n_win and j <= self.n_cols - self.n_win:\n",
    "                    if np.all(np.array([ self.board[i+k,j+k] == cur_p for k in range(self.n_win) ])):\n",
    "                        win = True\n",
    "            if not win:\n",
    "                if i <= self.n_rows - self.n_win and j >= self.n_win-1:\n",
    "                    if np.all(np.array([ self.board[i+k,j-k] == cur_p for k in range(self.n_win) ])):\n",
    "                        win = True\n",
    "            if win:\n",
    "                self.gameOver = True\n",
    "                return self.curTurn\n",
    "\n",
    "        if len(self.getEmptySpaces()) == 0:\n",
    "            self.gameOver = True\n",
    "            return 0\n",
    "\n",
    "        self.gameOver = False\n",
    "        return None\n",
    "\n",
    "    def printBoard(self):\n",
    "        for i in range(0, self.n_rows):\n",
    "            print('----'*(self.n_cols)+'-')\n",
    "            out = '| '\n",
    "            for j in range(0, self.n_cols):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('----'*(self.n_cols)+'-')\n",
    "\n",
    "    def getState(self):\n",
    "        return (self.getHash(), self.getEmptySpaces(), self.curTurn)\n",
    "\n",
    "    def action_from_int(self, action_int):\n",
    "        return ( int(action_int / self.n_cols), int(action_int % self.n_cols))\n",
    "\n",
    "    def int_from_action(self, action):\n",
    "        return action[0] * self.n_cols + action[1]\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.board[action[0], action[1]] != 0:\n",
    "            return self.getState(), -10, True, {}\n",
    "        self.makeMove(self.curTurn, action[0], action[1])\n",
    "        reward = self.isTerminal()\n",
    "        self.curTurn = -self.curTurn\n",
    "        return self.getState(), 0 if reward is None else reward, reward is not None, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
    "        self.boardHash = None\n",
    "        self.gameOver = False\n",
    "        self.emptySpaces = None\n",
    "        self.curTurn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4292653795.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_6542/4292653795.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    self.zeros_policy =\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class AgentQ:\n",
    "\n",
    "    def __init__(self, n_rows, n_cols, n_win, alpha=0.05, epsilon=0.1, gamma=0.99) -> None:\n",
    "        self.board_size = n_rows * n_cols\n",
    "        self.env = TicTacToe(n_rows=n_rows, n_cols=n_cols, n_win=n_win)\n",
    "        actions = [f\"{i}{j}\" for i in range(n_rows) for j in range(n_cols)]\n",
    "        self.q_zeros = defaultdict(lambda: {a: 0 for a in actions})\n",
    "        self.q_cross = defaultdict(lambda: {a: 0 for a in actions})\n",
    "        self.zeros_policy = defaultdict(lambda: np.random.choice(actions))\n",
    "        self.cross_policy = defaultdict(lambda: np.random.choice(actions))\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    @staticmethod\n",
    "    def empty_to_string(empty_spaces):\n",
    "        return [\"\".join(map(str, coor)) for coor in empty_spaces]\n",
    "\n",
    "    def update_cross_policy(self):\n",
    "        for state, action_dict in self.q_cross.items():\n",
    "            best_action = max(action_dict.items(), key=lambda x: x[1])[0]\n",
    "            self.cross_policy[state] = best_action\n",
    "\n",
    "    def update_zeros_policy(self):\n",
    "        for state, action_dict in self.q_zeros.items():\n",
    "            best_action = max(action_dict.items(), key=lambda x: x[1])[0]\n",
    "            self.zeros_policy[state] = best_action\n",
    "            \n",
    "    def get_action(self, state, empty_spaces, cur_turn):\n",
    "        spaces_strings = self.empty_to_string(empty_spaces)\n",
    "        if cur_turn == 1:\n",
    "            action = self.cross_policy[state] if random.random() > self.epsilon else random.choice(spaces_strings)\n",
    "        else:\n",
    "            action = self.zeros_policy[state] if random.random() > self.epsilon else random.choice(spaces_strings)\n",
    "        return action\n",
    "    \n",
    "    def step_zeros(self, state, empty_spaces):\n",
    "        \n",
    "        spaces_strings = self.empty_to_string(empty_spaces)\n",
    "        action = self.zeros_policy[state] if random.random() > self.epsilon else random.choice(spaces_strings)\n",
    "        (new_state, empty_spaces, cur_turn), reward_zeros, done, _ = env.step(action)\n",
    "\n",
    "        return new_state, empty_spaces, cur_turn, reward, -reward, done\n",
    "    \n",
    "    def update(self):\n",
    "        \n",
    "        self.env.reset()\n",
    "        state_cross, empty_spaces, cur_turn = env.getState()\n",
    "        action_cross = self.get_action(state_cross, empty_spaces, cur_turn)\n",
    "        (state_zeros, empty_spaces, cur_turn), reward_cross, done, _ = env.step(action)\n",
    "        action_zeros = self.get_action(state_zeros, empty_spaces, cur_turn)\n",
    "        (new_state_cross, empty_spaces, cur_turn), reward_zeros, done, _ = env.step(action_zeros)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning_episode(env, pi_cross, pi_zero, Q_cross, Q_zero, board_size, mapping, alpha=0.05, epsilon=0.0, gamma=0.9):\n",
    "    env.reset()\n",
    "    square_size = board_size ** 2\n",
    "    s_cross = mapping[env.getState()[0]]\n",
    "    a_cross = pi_cross[s_cross] if np.random.rand() > epsilon else np.random.randint(0, square_size)\n",
    "    _, reward_cross, _, _ = env.step(square_action(a_cross, board_size))\n",
    "    s_zero = mapping[env.getState()[0]]\n",
    "    a_zero = pi_zero[s_zero] if np.random.rand() > epsilon else np.random.randint(0, square_size)\n",
    "    for _ in range(9):\n",
    "        if env.curTurn == 1:\n",
    "            s_prime_zero, reward_cross, done, _ = env.step(square_action(a_cross, board_size))\n",
    "            if reward_cross == 1:\n",
    "                reward_zero = -1\n",
    "            s_prime_zero = mapping[s_prime_zero[0]]\n",
    "            a_prime_zero = pi_zero[s_prime_zero] if np.random.rand() > epsilon else np.random.randint(0, square_size)\n",
    "            Q_zero[s_zero, a_zero] = Q_zero[s_zero, a_zero] + alpha * (reward_zero + gamma * Q_zero[s_prime_zero].max() - Q_zero[s_zero, a_zero])\n",
    "            s_zero, a_zero = s_prime_zero, a_prime_zero\n",
    "            if done:\n",
    "                Q_cross[s_cross, a_cross] = reward_cross\n",
    "                break\n",
    "        else:\n",
    "            s_prime_cross, reward_zero, done, _ = env.step(square_action(a_zero, board_size))\n",
    "            if reward_zero == -1:\n",
    "                reward_zero = 1\n",
    "                reward_cross = -1\n",
    "            s_prime_cross = mapping[s_prime_cross[0]]\n",
    "            a_prime_cross = pi_cross[s_prime_cross] if np.random.rand() > epsilon else np.random.randint(0, square_size)\n",
    "            Q_cross[s_cross, a_cross] = Q_cross[s_cross, a_cross] + alpha * (reward_cross + gamma * Q_cross[s_prime_cross].max() - Q_cross[s_cross, a_cross])\n",
    "            s_cross, a_cross = s_prime_cross, a_prime_cross\n",
    "            if done:\n",
    "                Q_zero[s_zero, a_zero] = reward_zero\n",
    "                break\n",
    "    return Q_cross, Q_zero, reward_cross, reward_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = create_mapping(N_ROWS)\n",
    "Q_cross = init_Q(mapping, N_ROWS)\n",
    "Q_zero = init_Q(mapping, N_ROWS)\n",
    "pi_cross = update_policy(Q_cross, N_ROWS)\n",
    "pi_zero = update_policy(Q_zero, N_ROWS)\n",
    "env = TicTacToe(N_ROWS, N_COLS, N_WIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 1_500_000\n",
    "gamma = 0.9\n",
    "rewards_cross = []\n",
    "rewards_zero = []\n",
    "\n",
    "for n in range(total_episodes):\n",
    "    Q_cross, Q_zero, reward_cross, reward_zero = Q_learning_episode(\n",
    "        env=env, \n",
    "        pi_cross=pi_cross,\n",
    "        pi_zero=pi_zero, \n",
    "        Q_cross=Q_cross, \n",
    "        Q_zero=Q_zero,\n",
    "        board_size = N_ROWS,\n",
    "        mapping=mapping,\n",
    "        alpha=0.1, \n",
    "        epsilon= 0.2, \n",
    "        gamma=gamma\n",
    "    )\n",
    "    pi_cross = update_policy(Q_cross, N_ROWS)\n",
    "    pi_zero = update_policy(Q_zero, N_ROWS)\n",
    "    rewards_cross.append(reward_cross)\n",
    "    rewards_zero.append(reward_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        s = mapping[env.getState()[0]]\n",
    "        a = square_action(pi_cross[s], N_COLS)\n",
    "        _, reward, done, _ = env.step(a)\n",
    "        print(env.board)\n",
    "        if done and reward == 1:\n",
    "            print(\"You loose\")\n",
    "            break\n",
    "        elif done and reward == -10:\n",
    "            print(\"Bot debil\")\n",
    "            break\n",
    "        elif done:\n",
    "            print(\"Draw\")\n",
    "            break\n",
    "        user_a = input(\"Your turn: \")\n",
    "        action = tuple(map(lambda x: int(x) - 1, user_a.split()))\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        print(env.board)\n",
    "        if done:\n",
    "            print(\"You win\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "395bb32cbcecf10a878680820ab516cd6c613bfc5f39a5087bea9c6c2db8bbc7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
